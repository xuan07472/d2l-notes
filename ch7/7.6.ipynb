{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残差网络\n",
    "''' \n",
    "随着我们设计越来越深的⽹络，深刻理解“新添加的层如何提升神经⽹络的性能”变得⾄关重要。更重要的\n",
    "是设计⽹络的能⼒，在这种⽹络中，添加层会使⽹络更具表现⼒，为了取得质的突破，我们需要⼀些数学基\n",
    "础知识。\n",
    "-------------------------------------------------------------------------------------\n",
    "函数类\n",
    "\n",
    "只有当较复杂的函数类包含较⼩的函数类时，我们才能确保提⾼它们的性能。对于深度神经⽹络，如果我们\n",
    "能将新添加的层训练成恒等映射（identity function）f(x) = x，新模型和原模型将同样有效。同时，\n",
    "由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。\n",
    "\n",
    "针对这⼀问题，何恺明等⼈提出了残差⽹络（ResNet）[He et al., 2016a]。它在2015年的ImageNet图像识别\n",
    "挑战赛夺魁，并深刻影响了后来的深度神经⽹络的设计。残差⽹络的核⼼思想是：每个附加层都应该更容易\n",
    "地包含原始函数作为其元素之⼀。于是，残差块（residual blocks）便诞⽣了，这个设计对如何建⽴深层神\n",
    "经⽹络产⽣了深远的影响。凭借它，ResNet赢得了2015年ImageNet⼤规模视觉识别挑战赛。\n",
    "-------------------------------------------------------------------------------------\n",
    "残差块 VGG,结束了卷积核的战国时代\n",
    "\n",
    "ResNet沿⽤了VGG完整的3 × 3卷积层设计。残差块⾥⾸先有2个有相同输出通道数的3 × 3卷积层。每个卷积\n",
    "层后接⼀个批量规范化层和ReLU激活函数。然后我们通过跨层数据通路，跳过这2个卷积运算，将输⼊直接\n",
    "加在最后的ReLU激活函数前。这样的设计要求2个卷积层的输出与输⼊形状⼀样，从⽽使它们可以相加。如\n",
    "果想改变通道数，就需要引⼊⼀个额外的1 × 1卷积层来将输⼊变换成需要的形状后再做相加运算。残差块的\n",
    "实现如下：\n",
    "'''\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "我们在定义自已的网络的时候，需要继承nn.Module类，\n",
    "并重新实现构造函数__init__和forward这两个方法。但有一些注意技巧：\n",
    "\n",
    "1 一般把网络中具有可学习参数的层（如全连接层、卷积层等）放在构造函数__init__()中，\n",
    "当然我也可以把不具有参数的层也放在里面；\n",
    "\n",
    "2 一般把不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)可放在构造函数中，\n",
    "也可不放在构造函数中，如果不放在构造函数__init__里面，则在forward方法里面可以使用nn.functional来代替\n",
    "    \n",
    "3 forward方法是必须要重写的，它是实现模型的功能，实现各个层之间的连接关系的核心。\n",
    "\n",
    "详见文尾 \n",
    "'''\n",
    "class Residual(nn.Module): #@save\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels) # 属性 num_channels 输出通道\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "    def forward(self, X): # 方法\n",
    "        Y = F.relu(self.bn1(self.conv1(X))) # 卷积 批量规范化 激活\n",
    "        Y = self.bn2(self.conv2(Y)) # 卷积 批量规范化 后接残差的跳跃连接\n",
    "        if self.conv3: # 若需改变通道\n",
    "            X = self.conv3(X) \n",
    "        Y += X  # 将输入添加到输出\n",
    "        return F.relu(Y) # 激活"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "此代码⽣成两种类型的⽹络：⼀种是当use_1x1conv=False时，应⽤ReLU⾮线性函数之前，\n",
    "将输⼊添加到输出。另⼀种是当use_1x1conv=True时，添加通过1 × 1卷积调整通道和分辨率。\n",
    "'''\n",
    "# 下⾯我们来查看输⼊和输出形状⼀致的情况。\n",
    "blk = Residual(3,3) # 输入通道 3  输出通道 3\n",
    "X = torch.rand(4, 3, 6, 6) # 样本数 通道 高 宽\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们也可以在增加输出通道数的同时，减半输出的⾼和宽。\n",
    "blk = Residual(3,6, use_1x1conv=True, strides=2) # 输入通道 3 输出通道 6\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet模型\n",
    "''' \n",
    "ResNet的前两层跟之前介绍的GoogLeNet中的⼀样：在输出通道数为64、步幅为2的7 × 7卷积层后，接步幅\n",
    "为2的3 × 3的最⼤汇聚层。不同之处在于ResNet每个卷积层后增加了批量规范化层。\n",
    "'''\n",
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "GoogLeNet在后⾯接了4个由Inception块组成的模块。ResNet则使⽤4个由残差块组成的模块，每个模块使⽤\n",
    "若⼲个同样输出通道数的残差块。第⼀个模块的通道数同输⼊通道数⼀致。由于之前已经使⽤了步幅为2的\n",
    "最⼤汇聚层，所以⽆须减⼩⾼和宽。之后的每个模块在第⼀个残差块⾥将上⼀个模块的通道数翻倍，并将⾼\n",
    "和宽减半。\n",
    "下⾯我们来实现这个模块。注意，我们对第⼀个模块做了特别处理。\n",
    "'''\n",
    "def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                 first_block=False):\n",
    "    blk = [] # 块\n",
    "    for i in range(num_residuals): # 残差块数目\n",
    "        if i == 0 and not first_block: # 若是模块中的第一个残差块并且不是第一个残差模块\n",
    "            blk.append(Residual(input_channels, num_channels,\n",
    "                                use_1x1conv=True, strides=2)) # 使用1x1卷积改变c并将h,w减半\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels)) # 无需减少 h,w\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接着在ResNet加⼊所有残差模块(4个)，这⾥每个模块使⽤2个残差块\n",
    "# resnet_block(input_channels, num_channels, num_residuals,first_block=False)\n",
    "''' \n",
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "'''\n",
    "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True)) # 每个模块使用2个残差块\n",
    "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "b5 = nn.Sequential(*resnet_block(256, 512, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后，与GoogLeNet⼀样，在ResNet中加⼊全局平均汇聚层，以及全连接层输出。\n",
    "net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                    nn.AdaptiveAvgPool2d((1,1)), # 高 宽\n",
    "                    nn.Flatten(), nn.Linear(512, 10)) # 将512个通道视为512个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 128, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 256, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 512])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "每个模块有4个卷积层（不包括恒等映射的1 × 1卷积层）。加上第⼀个7 × 7卷积层和最后⼀个全连接层，共\n",
    "有18层。因此，这种模型通常被称为ResNet-18。通过配置不同的通道数和模块⾥的残差块数可以得到不同\n",
    "的ResNet模型，例如更深的含152层的ResNet-152。虽然ResNet的主体架构跟GoogLeNet类似，但ResNet架\n",
    "构更简单，修改也更⽅便。这些因素都导致了ResNet迅速被⼴泛使⽤。图7.6.4描述了完整的ResNet-18。\n",
    "\n",
    "在训练ResNet之前，让我们观察⼀下ResNet中不同模块的输⼊形状是如何变化的。在之前所有架构中，分辨\n",
    "率降低，通道数量增加，直到全局平均汇聚层聚集所有特征。\n",
    "'''\n",
    "X = torch.rand(size=(1, 1, 224, 224))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.016, train acc 0.996, test acc 0.911\n",
      "3103.0 examples/sec on cuda:0\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"180.65625pt\" viewBox=\"0 0 238.965625 180.65625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-10-20T09:29:44.510734</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 180.65625 \nL 238.965625 180.65625 \nL 238.965625 0 \nL 0 0 \nL 0 180.65625 \nz\n\" style=\"fill: none\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 143.1 \nL 225.403125 143.1 \nL 225.403125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 51.803125 143.1 \nL 51.803125 7.2 \n\" clip-path=\"url(#p57bc345070)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"mf2424aa160\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mf2424aa160\" x=\"51.803125\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 2 -->\n      <g transform=\"translate(48.621875 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 95.203125 143.1 \nL 95.203125 7.2 \n\" clip-path=\"url(#p57bc345070)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#mf2424aa160\" x=\"95.203125\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 4 -->\n      <g transform=\"translate(92.021875 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 138.603125 143.1 \nL 138.603125 7.2 \n\" clip-path=\"url(#p57bc345070)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mf2424aa160\" x=\"138.603125\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 6 -->\n      <g transform=\"translate(135.421875 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 182.003125 143.1 \nL 182.003125 7.2 \n\" clip-path=\"url(#p57bc345070)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#mf2424aa160\" x=\"182.003125\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 8 -->\n      <g transform=\"translate(178.821875 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 225.403125 143.1 \nL 225.403125 7.2 \n\" clip-path=\"url(#p57bc345070)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#mf2424aa160\" x=\"225.403125\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10 -->\n      <g transform=\"translate(219.040625 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- epoch -->\n     <g transform=\"translate(112.525 171.376563)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path d=\"M 30.103125 138.78991 \nL 225.403125 138.78991 \n\" clip-path=\"url(#p57bc345070)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path id=\"mcd5d12eea1\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mcd5d12eea1\" x=\"30.103125\" y=\"138.78991\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <g transform=\"translate(7.2 142.589129)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path d=\"M 30.103125 113.614271 \nL 225.403125 113.614271 \n\" clip-path=\"url(#p57bc345070)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#mcd5d12eea1\" x=\"30.103125\" y=\"113.614271\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(7.2 117.41349)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path d=\"M 30.103125 88.438632 \nL 225.403125 88.438632 \n\" clip-path=\"url(#p57bc345070)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#mcd5d12eea1\" x=\"30.103125\" y=\"88.438632\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 92.237851)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path d=\"M 30.103125 63.262994 \nL 225.403125 63.262994 \n\" clip-path=\"url(#p57bc345070)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#mcd5d12eea1\" x=\"30.103125\" y=\"63.262994\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 67.062212)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path d=\"M 30.103125 38.087355 \nL 225.403125 38.087355 \n\" clip-path=\"url(#p57bc345070)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#mcd5d12eea1\" x=\"30.103125\" y=\"38.087355\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 41.886573)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_21\">\n      <path d=\"M 30.103125 12.911716 \nL 225.403125 12.911716 \n\" clip-path=\"url(#p57bc345070)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use xlink:href=\"#mcd5d12eea1\" x=\"30.103125\" y=\"12.911716\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 16.710935)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_23\">\n    <path d=\"M 12.743125 14.959203 \nL 17.083125 49.297245 \nL 21.423125 63.232103 \nL 25.763125 71.113884 \nL 30.103125 76.690279 \nL 34.443125 104.472596 \nL 38.783125 106.058054 \nL 43.123125 106.399956 \nL 47.463125 106.706665 \nL 51.803125 106.765039 \nL 56.143125 114.575818 \nL 60.483125 114.430841 \nL 64.823125 114.672249 \nL 69.163125 114.438206 \nL 73.503125 114.442572 \nL 77.843125 122.134584 \nL 82.183125 120.943884 \nL 86.523125 120.447382 \nL 90.863125 120.507909 \nL 95.203125 120.253766 \nL 99.543125 126.380862 \nL 103.883125 125.634621 \nL 108.223125 125.038888 \nL 112.563125 124.712708 \nL 116.903125 124.648331 \nL 121.243125 129.795605 \nL 125.583125 129.911962 \nL 129.923125 129.486989 \nL 134.263125 129.196051 \nL 138.603125 128.552305 \nL 142.943125 132.672724 \nL 147.283125 132.492594 \nL 151.623125 131.551599 \nL 155.963125 131.724337 \nL 160.303125 131.580387 \nL 164.643125 134.708586 \nL 168.983125 134.732359 \nL 173.323125 134.832668 \nL 177.663125 134.528475 \nL 182.003125 133.836469 \nL 186.343125 136.068442 \nL 190.683125 136.237732 \nL 195.023125 135.990801 \nL 199.363125 136.041109 \nL 203.703125 135.49981 \nL 208.043125 136.766388 \nL 212.383125 136.922727 \nL 216.723125 136.707715 \nL 221.063125 136.708123 \nL 225.403125 136.725792 \n\" clip-path=\"url(#p57bc345070)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path d=\"M 12.743125 51.003679 \nL 17.083125 42.074404 \nL 21.423125 38.180815 \nL 25.763125 35.891391 \nL 30.103125 34.147367 \nL 34.443125 25.528829 \nL 38.783125 24.969114 \nL 43.123125 24.922036 \nL 47.463125 24.801723 \nL 51.803125 24.830283 \nL 56.143125 21.804374 \nL 60.483125 21.840991 \nL 64.823125 21.755552 \nL 69.163125 21.822683 \nL 73.503125 21.863753 \nL 77.843125 19.031957 \nL 82.183125 19.518438 \nL 86.523125 19.621314 \nL 90.863125 19.575979 \nL 95.203125 19.658787 \nL 99.543125 17.295273 \nL 103.883125 17.54636 \nL 108.223125 17.800934 \nL 112.563125 17.920375 \nL 116.903125 17.957334 \nL 121.243125 15.997991 \nL 125.583125 15.867217 \nL 129.923125 16.102611 \nL 134.263125 16.225539 \nL 138.603125 16.509734 \nL 142.943125 14.899487 \nL 147.283125 14.930872 \nL 151.623125 15.335401 \nL 155.963125 15.263039 \nL 160.303125 15.339067 \nL 164.643125 14.146226 \nL 168.983125 14.182843 \nL 173.323125 14.135764 \nL 177.663125 14.240384 \nL 182.003125 14.491487 \nL 186.343125 13.664976 \nL 190.683125 13.586512 \nL 195.023125 13.678926 \nL 199.363125 13.675438 \nL 203.703125 13.895664 \nL 208.043125 13.445275 \nL 212.383125 13.377273 \nL 216.723125 13.448763 \nL 221.063125 13.468815 \nL 225.403125 13.469776 \n\" clip-path=\"url(#p57bc345070)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path d=\"M 30.103125 35.506852 \nL 51.803125 28.394734 \nL 73.503125 38.69157 \nL 95.203125 29.401759 \nL 116.903125 26.45621 \nL 138.603125 24.983435 \nL 160.303125 25.726116 \nL 182.003125 24.404395 \nL 203.703125 24.756854 \nL 225.403125 24.102287 \n\" clip-path=\"url(#p57bc345070)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 143.1 \nL 30.103125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 225.403125 143.1 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 143.1 \nL 225.403125 143.1 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 140.634375 98.667187 \nL 218.403125 98.667187 \nQ 220.403125 98.667187 220.403125 96.667187 \nL 220.403125 53.632812 \nQ 220.403125 51.632812 218.403125 51.632812 \nL 140.634375 51.632812 \nQ 138.634375 51.632812 138.634375 53.632812 \nL 138.634375 96.667187 \nQ 138.634375 98.667187 140.634375 98.667187 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_26\">\n     <path d=\"M 142.634375 59.73125 \nL 152.634375 59.73125 \nL 162.634375 59.73125 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- train loss -->\n     <g transform=\"translate(170.634375 63.23125)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"264.550781\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"292.333984\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"353.515625\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"405.615234\"/>\n     </g>\n    </g>\n    <g id=\"line2d_27\">\n     <path d=\"M 142.634375 74.409375 \nL 152.634375 74.409375 \nL 162.634375 74.409375 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- train acc -->\n     <g transform=\"translate(170.634375 77.909375)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"264.550781\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"325.830078\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"380.810547\"/>\n     </g>\n    </g>\n    <g id=\"line2d_28\">\n     <path d=\"M 142.634375 89.0875 \nL 152.634375 89.0875 \nL 162.634375 89.0875 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- test acc -->\n     <g transform=\"translate(170.634375 92.5875)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"100.732422\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"152.832031\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"192.041016\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"223.828125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"285.107422\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"340.087891\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p57bc345070\">\n   <rect x=\"30.103125\" y=\"7.2\" width=\"195.3\" height=\"135.9\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练模型\n",
    "# 同之前⼀样，我们在Fashion-MNIST数据集上训练ResNet。\n",
    "lr, num_epochs, batch_size = 0.05, 10, 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "⼩结\n",
    "• 学习嵌套函数（nested function）是训练神经⽹络的理想情况。在深层神经⽹络中，学习另⼀层作为恒\n",
    "等映射（identity function）较容易（尽管这是⼀个极端情况）。\n",
    "• 残差映射可以更容易地学习同⼀函数，例如将权重层中的参数近似为零。\n",
    "• 利⽤残差块（residual blocks）可以训练出⼀个有效的深层神经⽹络：输⼊可以通过层间的残余连接更\n",
    "快地向前传播。\n",
    "• 残差⽹络（ResNet）对随后的深层神经⽹络设计产⽣了深远影响。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为什么深度网络（vgg，resnet）最后都不使用softmax（概率归一）函数，而是直接加fc层？\n",
    "''' \n",
    "softmax运算获取⼀个向量并将其映射为概率\n",
    "\n",
    "尽管softmax是⼀个⾮线性函数，但softmax回归的输出仍然由输⼊特征的仿射变换决定。因此，softmax回\n",
    "归是⼀个线性模型（linear model\n",
    "\n",
    "在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。通常我们使⽤预测概\n",
    "率最⾼的类别作为输出类别。如果预测与实际类别（标签）⼀致，则预测是正确的。在接下来的实验中，我\n",
    "们将使⽤精度（accuracy）来评估模型的性能。精度等于正确预测数与预测总数之间的⽐率。\n",
    "\n",
    "softmax回归是⼀个单层神经⽹络,由于计算每个输出o1、o2和o3取决于所有输⼊x1、x2、x3和x4，\n",
    "所以softmax回归的输出层也是全连接层。因此，为了实现我们的模型，我们只需在Sequential中\n",
    "添加⼀个带有10个输出的全连接层\n",
    "--------------------------------------------------------------------------------------------\n",
    "深度神经网络的最后一层往往是全连接层+Softmax(分类网络)\n",
    "\n",
    "全连接层将权重矩阵与输入向量相乘再加上偏置，将n个(−∞,+∞) 的实数映射为\n",
    "K个(−∞,+∞) 的实数（分数）；Softmax将K个(−∞,+∞) 的实数映射为K 个(0,1)的实数（概率），\n",
    "同时保证它们之和为1。具体如下：\n",
    "                y^=softmax(z)=softmax(WT x+b)\n",
    "\n",
    "通常将网络最后一个全连接层的输入视为网络从输入数据提取到的特征。\n",
    "通过对特征加权求和得到每个类别的分数，再经过Softmax映射为概率。\n",
    "\n",
    "softmax回归的输出层是⼀个全连接层。因此，为了实现我们的模型，我们只需\n",
    "在Sequential中添加⼀个带有10个输出的全连接层。\n",
    "--------------------------------------------------------------------------------------------\n",
    "为什么深度网络（vgg，resnet）最后都不使用softmax（概率归一）函数，而是直接加fc层？\n",
    "\n",
    "这个问题很简单，并不是没有使用softmax，而是没有显式使用softmax。\n",
    "随着深度学习框架的发展，为了更好的性能，部分框架选择了在使用交叉熵损失函数时默认加上softmax，\n",
    "这样无论你的输出层是什么，只要用了nn.CrossEntropyLoss就默认加上了softmax。\n",
    "不仅是Pytorch，国内的飞桨PaddlePaddle2.0等框架也是这样。但在更早的一些版本，\n",
    "默认是不隐式添加softmax的，所以会有一部分教程/资料，要求在撰写对应代码时手动添加softmax。\n",
    "当然，自己的框架是否需要手动在输出层添加softmax，推荐看对应的API文档。道听途说不一定可取，\n",
    "毕竟一个softmax可以做分类，俩softmax搞不好就不收敛了（逃）\n",
    "\n",
    "RESNET、VGG等网络的网络结构最后一层都是softmax函数。RESNET本质上就是VGG+shortcut。\n",
    "所以就只针对VGG进行讨论吧。就pytorch框架，VGG的损失函数使用的是nn.CrossEntropyLoss()，\n",
    "该损失函数结合了nn.LogSoftmax() 和 nn.NLLLoss()两个函数\n",
    "\n",
    "Pytorch中的nn.CrossEntropyLoss()函数的主要是将softmax-log-NLLLoss合并到一块得到的结果。\n",
    "\n",
    "    1、Softmax后的数值都在0~1之间，所以ln之后值域是负无穷到0。\n",
    "\n",
    "    2、然后将Softmax之后的结果取log，将乘法改成加法减少计算量，同时保障函数的单调性 。\n",
    "\n",
    "    3、NLLLoss的结果就是把上面的输出与Label对应的那个值拿出来去掉负号，再求均值。\n",
    "\n",
    "总结：并不是VGG网络结构中不含softmax这一层，而是集成到了Loss函数中做运算去了。\n",
    "\n",
    "Pytorch 网络结构 model.py，最后一层确实不是softmax。 \n",
    "以分类任务为例，最后一层是 nn.Linear()，\n",
    "因为 Pytorch 把后续的softmax等操作放到了nn.CrossEntropyLoss() 里面了。\n",
    "\n",
    "softmax被集成到了交叉熵损失里面，所以在网络最后不需要添加softmax\n",
    "\n",
    "softmax 和 cross entropy的结合实在太过常用，以至于很多框架在计算loss时都把他们集合在一个函数里了。\n",
    "但是在测试阶段是会接softmax的\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "torch.nn是专门为神经网络设计的模块化接口。nn构建于autograd之上，可以用来定义和运行神经网络。\n",
    "nn.Module是nn中十分重要的类,包含网络各层的定义及forward方法。\n",
    "定义自已的网络：\n",
    "    需要继承nn.Module类，并实现forward方法。\n",
    "    \n",
    "    一般把网络中具有可学习参数的层放在构造函数__init__()中，\n",
    "    不具有可学习参数的层(如ReLU)可放在构造函数中，也可不放在构造函数中(而在forward中使用nn.functional来代替)\n",
    "    \n",
    "    只要在nn.Module的子类中定义了forward函数，backward函数就会被自动实现(利用Autograd)。\n",
    "    \n",
    "    在forward函数中可以使用任何Variable支持的函数，毕竟在整个pytorch构建的图中，是Variable在流动。\n",
    "    还可以使用if,for,print,log等python语法.\n",
    "    \n",
    "注：Pytorch基于nn.Module构建的模型中，只支持mini-batch的Variable输入方式\n",
    "\n",
    "在使用pytorch的时候，模型训练时，不需要使用forward，\n",
    "只要在实例化一个对象中传入对应的参数就可以自动调用 forward 函数\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "经常写PyTorch模型的人会写：output = model(images)来进行前项传播，\n",
    "但是有没有仔细想过为啥这个image传入之后就能自动调用forward呢？\n",
    "---------------------------------------------------------------------------\n",
    "利用Python的语言特性，y = model(x)是调用了对象model的__call__方法，\n",
    "而nn.Module把__call__方法实现为类对象的forward函数，\n",
    "所以任意继承了nn.Module的类对象都可以这样简写来调用forward函数。\n",
    "\n",
    "调用forward方法的具体流程是：\n",
    "\n",
    "执行y = model(x)时，由于LeNet类继承了Module类，而Module这个基类中定义了__call__方法，\n",
    "所以会执行__call__方法，而__call__方法中调用了forward()方法\n",
    "\n",
    "只要定义类型的时候，实现__call__函数，这个类型就成为可调用的。\n",
    "换句话说，我们可以把这个类型的对象当作函数来使用\n",
    "\n",
    "总结：当执行model(x)的时候，底层自动调用forward方法计算结果\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('d2l': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbb412de504602cf3af13b2245b631c81b4c2841b1a325e0f1c2b48c8f82890b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
